## Practical Accuracy Estimation for Efficient Deep Neural Network Testing

提出**PACE** (**P**ractical **AC**curacy **E**stimation)方法，用于选择一小部分测试输入来精确估算整个测试输入集的准确性

###### 当前对Deep Neural Network (DNN)的测试主要存在两个挑战：

1. 由于实际场景的复杂多变，使用的测试输入是否足够测试DNN模型
2. 对测试集的测试输入进行标注的代价十分巨大

###### 目前现有的通过选择少量测试输入以达到精确估算测试集的准确度的方法CES，存在以下三个方面的不足：

1. CES选出的少量测试输入并没有很好的解释性（理解为为什么要这样选）
2. 由于CES存在随机性，它可能会生成不同的选择结果
3. CES的效用仍然有待提升

###### 针对CES中存在的局限性，PACE使用了以下方法来提升可解释性、确定性和高效性，同时能够提升估算测试集的准确度：

1. 使用聚类方法，将具有不同测试功能的测试输入划分为不同的簇
2. 使用MMD-critic算法，根据每个簇的大小从中选择簇的代表原型
3. 从minority space中随机选择测试输入，以实现测试输入选择的多样性

###### 主要贡献：

1. 提出了第一个解释性强，结果唯一，并且高效的方法PACE，来选择一小部分测试输入集来估算整个测试集的准确度
2. 使用了Keras、TensorFlow等框架和第三方库，对上述提出的方法进行了实现
3. 对基于24对待测试的模型和测试集进行了广泛研究
4. 对文中提到的方法的实现代码以及数据集进行了开源

###### PACE方法的具体实现：

***1.特征的选择：***

传统软件测试中，有**覆盖特征**和**输入特征**两种特征来反映测试能力，PACE采用了输入特征作为特征选择方法。理由如下：

1. 对于给定的DNN，许多测试输入可能具有非常相似的神经元覆盖范围，因此覆盖特征不能很好地区分其测试功能
2. 覆盖特征的效率比输入特征低
3. 收集覆盖特征会导致额外的代价

靠近输入层的隐含层有更基础的特征（basic features），靠近输出层的隐含层有更高阶的特征（high-order features）。high-order特征更能体现输入和输出之间的关系，但是获取的代价更大

PACE同时考虑了basic features和high-order features，具体包括：

* Original features（ORI）：输入向量，最基础的特征
* First-layer features（FL）：第一层隐含层，离输入向量最近的一层
* Last-hidden-layer features（LHL）：最后一层隐含层，离输出向量最近
* Confidence features（CON)：DNN的输出层，即分类器，体现预测结果的准确度

***2.基于聚类的分类方法***

依据特征选择，每个测试输入数据都被转化成了特征向量，然后PACE就可以通过特征向量将测试输入聚类为不同的簇

首先使用min-max normalization归一化方法将特征值映射到[0,1]之间，公式如下：

$x_{ij}^*=\frac{x_{ij}-min(\{x_{kj}|1\le k\le st\})}{max(\{x_{kj}|1\le k\le st\})-min(\{x_{kj}|1\le k\le st\})}$

然后使用HDBSCAN算法聚类归一化后的数据，HDBSCAN算法是DBSCAN算法的改进版本，是一种基于密度的聚类算法，过程如下：

1. 构造一个权值图，将每一个数据点作为图中的一个顶点，且每两个顶点之间有一条边，边的权值使用mutual reachability distance：$MRD_k(a,b)=max\{Core_k(a),Core_k(b),Dist(a,b)\}$，其中$Core_k(a)$和$Core_k(b)$分别表示和顶点$a$、$b$第$k$近的顶点的距离，$Dist(a,b)$是点$a$、$b$之间的欧几里得距离
2. 使用Prim构造最小生成树
3. 通过不断减小最小聚类大小来将树压缩为更小的树
4. 通过计算每个簇的stability score，从压缩后的树中提取稳定的簇

$p.s.$ 通过对HDBSCAN的学习了解，减小最小聚类大小，即减少每个簇中的顶点个数，因此会导致簇内的边断裂，使得当前簇分裂为更多的子簇，即达到了压缩为更小的树的目的

由于特征的维度过高会导致HDBSCAN的性能降低，因此在执行HDBSCAN之前，先用FastICA算法对特征进行降维

***3.基于MMD-critic的原型选择***

将测试输入分类后，PACE根据每个类中的元素数量从类中选取类的原型，以构造一个小的测试输入集

PACE使用MMD-critic算法从每个组中选择最能代表当前类特征的元素，以降低类中噪声的影响，并提升方法的解释能力

MMD-critic通过计算原型分布与组分布的差异来选择原型，选取的原型能够使得差异最小，通过对函数空间$F$上关于两个分布的期望之间的差异的上界给出，公式如下：

$MMD(F,P,G)=\underset{f\in F}{sup}(\mathbb{E}_{X\sim P}[f(X)]-\mathbb{E}_{Y\sim G}[f(Y)])$

用这种方法，PACE从每个组中选择指定数量的原型来构成测试输入集。除此之外，PACE还从每个组中找出效果最差的元素，以方便对组内测试输入空间的理解

***4.Minority Space的自适应探索***

PACE将不属于任何组的测试输入的集合成为少数空间（Minority Space），为了更好地解释方法，从少数空间中也选择一部分测试输入加入测试输入集

由于Minority Space中的测试输入和其他的测试输入之间的特征差异较大，因此很难从其中选出具有代表性的测试输入。所以采用自适应随机选择方法，以选择指定数量的多样性测试输入

算法首先选择特征最为独立的一个测试输入加入测试输入集，即与所有的组距离最大的一个测试输入。然后计算每一个未被选择的测试输入与已被选择的测试输入之间的欧几里得距离，并将与所有以备选择的测试输入距离的最小值作为当前输入与已选择输入集合的距离，然后从所有未被选择的测试输入中选择距离最大的一个作为下一个被选中的测试输入，并加入测试输入集。定义$U$为未被选中的测试输入集合，$S$为已经被选中的测试输入集合，则每次选择的公式如下：$d=\underset{y\in U}{max}\{\underset{x\in S}{min}\ EucDist(x,y)\}$

###### 提出五个RQ：

* PACE的有效性及其效率如何
* 不同的特征对PACE的影响
* 不同的阈值$\alpha$对PACE的影响（阈值$\alpha$即在每个组（包括Minority Space）中选择加入测试输入集的比例）
* 使用HDBSCAN时，降维操作（包括不同维度和不同降维算法）对PACE的影响
* 是否每一个部分（聚类、原型选择、自适应随机选择）都对PACE有贡献

###### 展望：

1. 在特征选择时，LHL层和ORI层分别在某些测试中表现地优于对方，因此考虑将两层合并是否能提升方法的有效性
2. HDBSCAN算法的代价比较大，导致和baseline比较时效率较低，因此考虑是否可以找到代价较小的聚类算法以提升PACE的效率。此外，由于噪声的影响，HDBSCAN算法可能会导致很多输入不能被聚类入任何一个簇，导致PACE性能较低（只是存在这种可能性）
3. 当前的PACE只考虑了精确评估整个测试集这一个指标，然而实际可能会有多个指标，以后会考虑将PACE应用于多目标
4. 在研究过程中仅将PACE应用于图片分类、自动驾驶和语音文本转换领域，而实际上PACE可以应用于更多的领域（例如自然语言处理），以后会在更多的领域测试PACE



