## Machine Learning Testing: Survey, Landscapes and Horizons

###### 一篇与机器学习测试有关的综述，研究了144篇论文，包括对属性、组件、工作流和应用场景的测试，同时分析了机器学习测试相关的数据集、研究方向的趋势，以及研究的关注点和研究所面临的挑战

##### 与机器学习测试相关的论文主要包括四个方面：

* 测试属性（testing properties）：正确性、鲁棒性和公平性（what to test）
* 机器学习组件（machine learning framework）：数据、程序、框架（where to test）
* 测试工作流（testing workflow）：测试生成、运行测试、测试评估（how to test）
* 应用场景（application scenarios）：自动驾驶、机器翻译

##### 文章的贡献：

* 定义了机器学习测试，概述与机器学习测试相关的概念、测试工作流程、测试属性以及测试组件
* 对144篇论文进行全面研究，交叉了软件工程、人工智能、系统与网络以及数据挖掘等多个领域
* 分析结果显示，约120篇论文基于监督学习，3篇基于无监督学习，1篇基于强化学习；93篇论文关注正确性与鲁棒性，只有少部分论文测试可解释性、隐私性和效率
* 分析了机器学习测试的挑战，以及开放性问题和研究方向，以促进进一步的研究

###### 机器学习分为三类：监督学习、无监督学习、强化学习

###### 机器学习可被应用于：分类、回归、聚类、降维（降低训练的复杂度，如数据预处理）、控制（控制行为以获取最大奖励，如Alpha Go）

###### 机器学习bug和机器学习测试表明了机器学习的三个方面：required conditions（测试属性）、machine learning items（机器学习组件）、testing activities（测试工作流）

###### 机器学习主要被应用于自动驾驶领域、机器翻译领域和自然语言推论

---

##### 机器学习测试工作流：

###### 包括离线测试$(offline\ testing)$与在线测试$(online\ testing)$

###### 在线测试方法如：$A/B\ testing、MAB (Multi-Armed\ Bandit)$，目前的研究主要集中在离线测试

**1.** ***测试输入生成$(Test\ Input\ Generation)$***：

* *指定领域的测试输入生成*

###### 对抗性输入（adversarial inputs）：不属于正常数据的分布，甚至很少出现在实际生活中，但是往往能够暴露鲁棒性和安全性的问题

###### 正常输入（normal inputs）：服从正常的数据分布

$DeepXplore$：提出一种白盒差异测试技术来生成<u>*深度学习系统*</u>的正常测试输入，生成的策略是尽可能提高神经元覆盖指标（分别比随机选择输入和对抗性输入的神经元覆盖率高$34.4\%$和$33.2\%$）

$DeepTest$：为<u>*自动驾驶系统*</u>生成有效数据，使用九种图像转换（改变亮度，改变对比度，平移，缩放，水平剪切，旋转，模糊，起雾效果和下雨效果）进行贪心搜索生成（使用$Udacity$自动驾驶数据集，在误报率较低的$CNN$和$RNN$上检测到1,000多种错误行为）

$Generative\ Adversarial\ Networks (GANs)$：用于生成模型，可以估计给定数据集上的分布

$DeepBillboard$：生成现实世界的<u>*对抗性广告牌*</u>，这些广告牌会触发<u>*自动驾驶系统*</u>的潜在转向错误

$Deepcruiser$：生成音频输入的转换，用于测试<u>*基于音频的深度学习系统*</u>，考虑了背景噪声和音量变化。他们首次从$RNN$中提取了概率转移模型，在此基础上，有状态测试标准被定义，并用于指导有状态机器学习系统的测试生成

$Cell\ Morphology\ Assay(CMA)$：在对<u>*生物细胞图像*</u>进行分类时<u>*测试图像分类平台*</u>，用于生物细胞分类器的测试框架

$Testing\ Neural\ Program\ Analyzers$：使用保留语义的程序转换作为测试输入来测试$code2vec$的可能性

$TransRepair$：测试<u>*机器翻译系统*</u>，变异翻译输入中的单词来自动生成测试输入，基于词嵌入相似性进行词替换，测试生成在生成具有一致翻译的输入对时具有$99\%$的精度

* *模糊与基于搜索的测试输入生成*

###### 模糊测试是一种自动测试技术，生成随机数据作为程序输入来检测崩溃、内存泄漏、失败断言等问题，应用于系统安全性和漏洞检测

###### 基于搜索的测试生成通常使用元启发式搜索技术来指导模糊过程，以更有效地进行测试生成

$TensorFuzz$：使用一种简单的最近邻爬山算法来探索$Tensorflow\ graph$在有效输入空间上的覆盖范围，并在神经网络及其量化版本之间发现数值型$bug$，在$RNN$中发现非预期的行为

$DLFuzz$：基于$DeepXplore$并以神经元覆盖为指标的模糊测试生成工具，目的是生成对抗样本，通过修改少量原始输入来寻找能有更高的神经元覆盖率并且与原始输入有更大差异的新输入

$DeepHunter$：基于变态变换的以覆盖率为指标的模糊技术，使用更细粒度的变异策略来生成测试，以减少$false\ positive$，在实现高覆盖率以及错误检测方面有优势

$Feature\ Guided\ Test\ Generation$：基于特征指导的测试生成方法，使用$Scale\ Invariant\ Feature\ Transform(SIFT)$来识别代表“图像具有高斯混合模型”的特征，然后将寻找对抗样本的问题转化为两人回合制随机游戏；使用“蒙特卡洛树搜索”（Monte Carlo Tree Search）来识别图像中用于生成对抗样本的元素

$Regorous\ Agent\ Evaluation$：使用对抗样本生成来评估<u>*强化学习*</u>，使用故障概率预测器来估计代理发生故障的概率

$DriveFI$：研究了如何通过将故障注入建模为贝叶斯网络来生成最有效（触发安全问题）的测试用例

$OGMA$：生成用于文本分类任务的输入，并产生一种结合语法与输入间距的模糊测试方法

* *基于符号执行的测试输入生成*

###### 符号执行是一种程序分析技术，用于测试软件是否可能违反某些属性。机器学习模型的性能是由代码和数据决定的，即符号执行的两个应用场景

###### 动态符号执行$(Dynamic\ Symbolic\ Execution,\ DSE)$，也称为$concolic$测试，是一种用于自动生成实现高代码覆盖率的测试输入的技术

$Integrating\ Symbolic\ and\ Statistical\ Methods$：使用符号和统计结合的方法，生成更高效且能暴露$bug$的测试用例。使用符号从距离理论上抽象数据，以搜索”微小变动会导致算法失败“的测试输入

###### 在机器学习代码上应用符号执行的挑战：

1. 网络没有明确的分支
2. 网络可能是高度非线性的，没有完善的约束求解器
3. 存在$scalability$问题，因为机器学习模型结构的复杂性，往往超出符号推理工具的能力

$DeepCheck$：将$DNN$转换为程序，使符号执行能够查找和原始图像激活模式相同的像素攻击，$DNN$中的激活函数遵循$IF-Else$分支结构，可以当做程序中的一条路径。$DeepCheck$能通过识别神经网络分类失败的图像中的大部分像素或像素对来创建$1-pixel$和$2-pixel$像素攻击

$Automated\ Test\ Generation\ to\ Detect\ Individual\ Discrimination\ in\ AI\ Models$：使用$LIME$（一种本地解释工具）来对具有线性模型、决策树或下降规则列表的模型进行近似，以获得用于符号执行的路径

$DeepConcolic$：是$DNN$的动态符号执行测试方法，将覆盖率需求作为输入，通过具体评估机器学习模型的给定属性，将符号分析引导至特定的$MC/DC$标准条件

* *使用综合数据测试学习程序*

$Murphy$等人生成的数据具有重复值、缺失值或分类数据来测试两个机器学习排名应用程序

$Breck$等人使用遵循模式约束的综合训练数据来触发代码中与约束不一致的隐藏假设

$Zhang$等人使用已知分布的合成数据来测试过拟合

$Nakajima$和$Bui$提及生成具有某些可预测特征的简单数据集的可能性，这些数据集可以用作伪预言

**2.** ***测试预言$(Test\ Oracle)$***

###### $Test\ Oracle$用来判断程序中是否存在错误，由于许多机器学习算法都是概率程序，因此$Oracle$问题很有挑战性，几种主流$Oracle$：蜕变关系、交叉引用、模型评估指标

* *蜕变关系$(Metamorphic\ Relations)$*

###### 蜕变关系是指在多个程序执行期间，软件输入变化和输出变化之间的关系

例如：在测试$sin(x)$函数的实现时，可以检查输入从$x$变化为$\pi-x$时输出的变化，如果$sin(x)$和$sin(\pi-x)$的值不同，则表明函数存在错误，而无需检查$sin(x)$函数的特定值是否正确。$sin(x)=sin(\pi-x)$就是用来测试预言的一种蜕变关系

###### 根据数据转换的粒度不同，将其分为*粗粒度数据转换$(Coarse-grained\ Data\ Transformation)$*和*细粒度数据转换$(Fine-grained\ Data\ Transformation)$*

粗粒度数据转换如：扩大数据集或更改数据顺序，而不会更改每个数据实例

细粒度数据转换如：对图像的属性，标签或像素进行突变

$Amsterdam$：使用蜕变关系自动检测机器学习中的$bug$，通过在进行结果比较时设置阈值来减少$false\ positive$

* *交叉引用$(Cross-Referencing)$*

###### 交叉引用包括差异测试$(Differential\ Testing)$和$N$版本编程$(N-version\ Programming)$

差异测试：差异测试是一种传统的软件测试技术，它通过观察相似的应用程序在相同输入下是否产生不同输出来检测错误，是用于检测编译器错误的测试预言。根据$Nejadgholi$和$Yang$的研究，$5\%$至$27\%$的深度学习库使用差异测试作为测试预言

$N-version$编程：旨在基于一个规范生成多个功能等效的程序，从而使不同版本的组合更具容错性和鲁棒性

$Davis$和$Weyuker$讨论了对“不可测”程序进行差异测试的可能性，如果算法的多个实现在一个相同的输入上产生不同的输出，则至少有一个实现包含缺陷

$Alebiosu$等人在机器学习中评估了这个想法，并成功地从7个朴素贝叶斯实现中发现了16个错误，从19个$k-nearest\ neighbor$实现中发现了13个错误

$CRADLE$：是第一个用于在$Deep\ Learning\ Library$中发现和定位错误的方法。使用三个库$(TensorFlow、CNTK、Theano)$，11个数据集$(ImageNet、MNIST、KGS\ Go\ game)$等以及30个预训练模型，检测到104个$unique\ inconsistency$和12个$bug$

$DeepXplore$和$DLFuzz$使用差异测试作为测试预言，以找到有效的测试输入，优先选择在不同算法或模型之间导致不同行为的测试输入作为生成的测试输入

$Qin$等人根据训练数据生成镜像程序作为伪预言，镜像程序基于训练数据，因此该程序的行为代表训练数据，如果镜像程序对测试数据具有类似的行为，则表明从训练数据中提取的行为也适合于测试数据

$TransRepair$：将交叉引用应用于维修机器翻译系统，比较多个变异的输入产生的

输出（即翻译），并选择与其他输入最相似的输出作为高级翻译候选

* *设计测试预言的度量标准*

非功能性度量特征不是直接的测试预言，但是能帮助测试人员了解和评估被测属性

有些特定领域的规则被应用于测试预言，比如$Kang$等人提出的两种汽车检测任务下的模型断言，

在终生学习中， 如果机器学习系统的知识水平能够随着时间增长而提升，则它可以通过测试预言

**3.** ***测试充分性$(Test\ Adequacy)$***

###### 测试充分性旨在发现当前的测试是够有较好的暴露错误的能力，它提供了对测试活动的客观置信度度量

###### 测试充分性的技术主要包括测试覆盖$(Test\ Coverage)$和变异测试$(Mutation\ Testing)$

* *测试覆盖*

###### 传统软件中以代码覆盖率衡量测试程序对程序源代码执行的程度，而机器学习中模型的决策逻辑不是手动编写，而是从训练数据中学习的，因此研究人员提出了新的覆盖标准

神经元覆盖$(Neuron\ Coverage)$：用于深度学习测试，表示$DNN$模型中被所有测试输入激活的神经元数量占总神经元数的比例（如果神经元的输出超出设置的阈值则）

$Ma$等人提出更细粒度的标准：$k-multisection\ neuron\ coverage$、$neuron\ boundry\ coverage$、$strong neuron\ activation\ converage$

$MC/DC$覆盖变体$(MC/DC\ Coverage\ Variants)$：是根据$MC/DC$覆盖标准，针对$DNN$的特性提出的测试覆盖标准，$MC/DC$观察布尔变量的变化，而变体通过观察神经元的符号、值、距离的变化来捕获测试输入的因果变化。其假设$DNN$是全连接网络，并且不考虑层内神经元的关系

层级覆盖$(Layer-level\ Coverage)$：通过考虑$top\ hyperactive$神经元及其组合来表征$DNN$的行为，比神经元覆盖性能更好。进一步提出组合测试覆盖率，通过检查层内神经元激活的相互作用的比例来检查神经元的组合激活状态

状态级覆盖$(State-level\ Coverage)$：$Du$等人提出$Deepcruiser$方法，针对$RNN$的测试标准，将有状态深度学习系统抽象为概率转移系统，基于状态转移系统的$state$和$trace$来捕获动态状态转移行为

###### 深度神经网络的结构覆盖标准的局限性是由神经网络与人工编写程序之间的根本差异引起的，测试集中输入错误的输入数量与其结构覆盖率之间没有强相关性

* *变异测试*

###### 传统软件测试中，变异测试通过注入错误来评估测试程序暴露错误的能力，检测到的错误与注入错误的比率称为$Mutation\ Score$

###### 机器学习系统的行为不仅取决于代码，还取决于数据和模型结构

$DeepMutation$：在原代码或模型中对$DNN$进行变异以使决策边界受到较小干扰，$Mutation\ Score$定义为结果发生变化的测试实例数与总实例数的比例

###### 与结构覆盖标准相比，变异测试更直接地与决策边界相关，即接近决策边界的输入数据更容易检测$DNN$和变异体之间的不一致性

* *惊奇充分性$(Surprise\ Adequacy)$*

衡量深度学习系统离散输入惊奇范围的覆盖范围，好的测试输入应足够但不要过分惊奇

提出了两种惊奇度量：

1. 基于核密度估计$(Keneral\ Density\ Estimation,\ KDE)$来估计系统在训练过程中看到相似输入的可能性
2. 基于“代表神经元激活轨迹的“<u>给定输入的</u>向量与<u>训练数据</u>向量之间的距离，可使用欧几里得距离等

未来工作可研究“对抗样本”、“自然错误样本”、“基于惊奇的标准”之间的关系

* *基于规则的测试充分性检查$(Rule-based\ Checking\ of\ Test\ Adequacy)$*

需要制定一些规则来确保机器学习系统的功能，$Breck$等人提出28个需要考虑的方面，分为四种类型：

1. 对机器学习模型的测试
2. 用于构建模型的基础结构的测试
3. 用于构建模型的数据的测试
4. 对机器学习系统是否一致正常工作的测试

**4.** ***测试优先级和压缩$(Test\ Prioritisation\ and\ Reduction)$***

###### 机器学习测试成本较高的原因：测试输入生成需要覆盖很大的输入空间；需要标记每个测试实例以判断预测准确性

$Byun$等人使用交叉熵等$DNN$度量对测试输入进行优先级排序，以暴露有$unacceptable$行为的输入，并且能被用于再训练

$Zhang$等人通过识别更有效的对抗性样本来降低成本，即根据测试实例对噪声的敏感度进行排名，因为对噪声敏感的实例更容易产生对抗性样本

$Li$等人使用基于交叉熵最小化的分布近似技术，提出了一种在$DNN$的最后一个隐藏层中由神经元引导的采样技术，以减少测试数据

$Ma$等人基于模型置信度提出一组测试选择指标，选择对模型更不确定的测试输入，因为它们的信息量更大

**5.** ***错误报告分析$(Bug\ Report\ Analysis)$***

$Thung$等人通过分析机器学习系统的错误报告来研究机器学习错误，对$Apache\ Mahout$、$Apache\ Lucene$、$Apache\ OpenNLP$中的500个错误报告进行分析，$22.6\%$的错误由于已定义算法的不正确实现，$15.6\%$的错误是非功能性错误，$5.6\%$的错误是数据错误

$Zhang$等人从$Github$和$StackOverflow$的错误报告中收集了175个$TensorFlow\ bug$，以研究$bug$发生的根源以及$bug$的解决方案。将$TensorFlow\ bug$分为：异常或崩溃、低正确率、低效率以及未知四类。主要是由算法的设计和实现引起的，例如$TensorFlow\ API$的滥用约$18.9\%$，$Tensor$不对齐占$13.7\%$，模型参数或结构错误占$21.7\%$

$Banerjee$等人分析了12个自动驾驶系统的错误报告，他们使用NLP将脱离（导致车辆控制从软件切换到驾驶员的故障）的原因分为10种，其中机器学习系统和决策控制中的问题占$64\%$

**6.** ***调试和修复$(Debug\ and\ Repair)$***

* 数据重采样$(Data\ Resampling)$

测试数据生成可以暴露机器学习的$bug$，也可以通过再训练来提高模型的正确性。$DeepXplore$通过再生成的输入上对深度学习模型进行再训练，将分类准确性提高$3\%$，$DeepTest$将模型准确性提高$46\%$

$Ma$等人识别出导致错误分类的故障神经元$faulty\ neuron$，对影响故障神经元的训练数据进行了重采样，以帮助改善模型性能

* 调试框架开发$(Debugging\ Framework\ Develpoment)$

$Dutta$等人提出了$Storm$，一种用于生成小程序，以支持机器学习测试的调试的程序转换框架

$Cai$等人提出了$tfdbg$，一个基于$TensorFlow$的调试器

$Vartak$等提出了$MISTIQUE$系统来捕获、存储和查询模型中间体以帮助调试

$Krishnan$和$Wu$ 提出$PALM$，包括划分训练数据的元模型，以及近似每个分区内模式的一组子模型两部分，可以找出对预测影响最大的训练数据，从而将考虑错误预测的训练数据子集作为目标，以协助调试

**7.** ***通用测试框架和工具$(General\ Testing\ Framework\ and\ Tools)$***

$Dreossi$等人提出一个$CNN$测试框架，由图像生成器，采样方法集合、可视化工具三个主要模块组成

$Tramer$等人提出一种全面的测试工具，通过错误报告来帮助开发人员测试和调试公平性错误

$Nishi$等人提出一个测试框架，包括容许性，可实现性，健壮性等评估方面

---

**机器学习测试组件：**

###### 包括测试数据、学习程序和框架中的bug，由于机器学习各组件之间的联系紧密，因此错误的传播是很严重的问题

* ***数据的错误检测$(Bug\ Detection\ in\ Data)$***

###### 机器学习系统的性能很大程度上取决于数据，因此要对数据进行测试。要尽早发现$data\ bug$，因为经过训练的模型通常会被记录并用于生成更多数据，随着时间的推移会放大微小的数据错误

**1.** *训练数据$(Bug\ Detection\ in\ Training\ Data)$*

1. 基于规则的$Data\ Bug$检测$(Rule$-$based)$：

    $Hynes$等人提出了一种自动检查机器学习数据集的工具$data\ linter$，考虑一下三种数据问题：

    - 数据编码错误$(miscoded\ data)$，例如将数字或日期误认为是字符串
    - 异常值和缩放$(outliers\ and\ scaling)$，例如不正常的$list$长度
    - 打包错误$(packaging\ errors)$，例如重复值、空样本和其他数据组织问题

    $Cheng$等人提出了一系列衡量训练数据是否涵盖所有重要场景的指标

2. 基于性能的$Data\ Bug$检测$(Performance$-$based)$：

    $MODE$：在神经网络中识别导致分类错误的“$faulty\ neurons$”，并通过数据重采样来测试训练数据，以分析$faulty\ neurons$是否受到影响

**2.** *测试数据$(Bug\ Detection\ in\ Test\ Data)$*

###### 测试数据中的对抗样本会带来安全风险，因此检测对抗样本类似于$bug$检测，测试数据不充足会导致无法检测过拟合的问题，因此也被认为是$Data\ Bug$

$Metzen$等人建议使用小型子网来增强$DNN$，该子网用于区分真实数据与对抗性扰动数据

$Wang$等人使用$DNN$模型变异来揭示对抗性示例，因为对抗样本对扰动更敏感

$Carlini$和$Wagner$发现对抗样本的检测方法依赖于损失函数，因此在构建新的损失函数时容易绕开对抗样本，对抗样本要更难检测

**3.** *训练数据和测试数据的倾斜检测$(Skew\ Detection\ in\ Training\ and\ Test\ Data)$*

###### 训练实例和预测的实例应该有一致的特征和分布

$Kim$等人提出了两种测量方法来评估训练数据和测试数据之间的偏斜：一种是基于核密度估计$(Kernel\ Density\ Estimation,KDE)$，另一种基于向量之间的距离

$Breck$研究训练数据和服务数据，即模型部署后的预测数据，之间的倾斜

**4.** *测试框架$(Frameworks\ in\ Detecting\ Data\ Bugs)$*

$Breck$等人提出了一种用于检测数据错误的数据验证系统

$Krishnan$等人提出模型训练框架$ActiveClean$，对数据进行迭代清理，根据数据对模型的价值，以及数据中存在"脏数据"的可能性来抽取数据样本

$Krishnan$等人提出$BoostClean$系统，用于检测训练数据中违反域值的情况

$Schelter$等人研究大型数据集的<u>*自动*</u>单元测试，提供了一个声明性$API$，结合常见的以及用户定义的质量约束以进行数据测试

$Krishnan$和$Wu$提出$AlphaClean$以进行<u>*自动*</u>数据清理

* ***学习程序的错误检测$(Bug\ Detection\ in\ Learning\ Program)$***

###### 学习程序中的错误检测主要是检查算法的实现和配置是否正确，是否存在编码错误

**1.** *机器学习程序的单元测试$(Unit\ Tests)$*

$McClure$引入了带有$TensorFlow$内置测试功能的机器学习单元测试

$Schaul$等人开发了一组专门为随机优化设计的单元测试

**2.** *算法配置检查$(Algorithm\ Configuration\ Examation)$*

$Sun$等人从三个机器学习框架$(Skikitlearn、Paddle、Caffe)$中研究了329个实际错误，由于不兼容的操作系统、语言版本或与硬件的冲突，超过$22\%$的错误是兼容性问题

$Guo$等研究了深度学习框架$(TensorFlow、Theano、Torch)$，他们使用不同的模型分类数据集$(MNIST、CIFAR$-$10)$对学习准确性，模型大小，鲁棒性进行比较

$Zhang$等人的研究表明，未正确使用$Tensorflow\ API$是最常见的学习程序错误，在基于$TensorFlow$的研究中，有$23.9\%\ (38/159)$的$bug$是学习程序中的$bug$

**3.** *算法选择检查$(Algorithm\ Selection\ Examation)$*

###### 开发人员通常有多种学习算法可供选择，不同的算法适用于不同的场景

$Fu$和$Menzies$在链接$Stack\ Overflow$问题的任务上比较了深度学习和分类学习，发现分类学习算法可以以较低的成本获得相似甚至更好的结果

$Liu$等人的工作发现在提交消息生成任务中，k近邻算法$(k$-$Nearest\ Neighbours)$与深度学习结果相似

**4.** *学习程序错误的变异模拟$(Mutant\ Simulations\ of\ Learning\ Program\ Faults)$*

$Murphy$等人使用了代码变异模拟代码错误，以研究所提出的蜕变关系在错误检测方面是否有效，并提出三种变异运算符：切换比较运算符$(switching\ comparison\ operators)$，数学运算符$(mathematical\ operators)$和循环变量的一对一$(off$-$by$-$one)$错误

$Dolby$等人扩展了$WALA$，以支持对$Tensor$行为的静态分析($Python$的$TensorFlow$程序)

* ***框架的错误检测$(Bug\ Detection\ in\ Framework)$***

**1.** *框架错误的研究$(Study\ of\ Framework\ Bugs)$*

$Xiao$等人对深度学习框架$(Caffe、TensorFlow、Torch)$的安全漏洞的研究发现，框架的依赖性很复杂，最常见的漏洞是导致程序崩溃、内存耗尽

$Guo$等人在相同的算法设计和配置下，测试不同深度学习框架$(TensorFlow、Theano、Torch)$的运行时行为、训练准确性和鲁棒性，发现每种框架的运行时训练行为都不同，但预测准确性仍然相似

$Sun$等人发现，用户报告的框架错误中大约有$10\%$与效率低有关

**2.** *框架实现错误的检测$(Implementation\ Testing\ of\ Frameworks)$*

###### 机器学习框架中的实现错误可能不会导致崩溃、错误或效率问题，从而使检测具有挑战性

$Thung$等人对三个机器学习系统的500个错误报告研究显示，大约$22.6\%$的错误由算法实现错误所致

$Alebiosu$等人在10个朴素贝叶斯实现中发现了5个错误，在20k最近邻实现中发现了4个故障

$Pham$等人在三个库$(TensorFlow、CNTK、Theano)$，11个数据集$(ImageNet、MNIST、KGS\ Go)$和30个预训练模型中发现了12个错误

并非每种算法都有多中实现，因此$Murphy$等人将蜕变关系应用于机器学习实现，列出不会导致输出变化的数据转换，例如将数值乘以常数、置换或反转输入数据、添加其他数据，发现提出的蜕变关系存在于三个机器学习的应用程序中

$Dwarakanath$等人应用了变形关系来发现图像分类中的实现错误

---

**机器学习测试属性：**

###### 包括功能需求（正确性、模型相关性）和非功能需求（有效性、鲁棒性、公平性、可解释性）等

* ***正确性$(Correctness)$***

$E(h)=Pr_{x\sim\mathcal{D}}[h(x)=c(x)]$，其中$\mathcal{D}$是未知数据，$h$是测试的模型，$h(x)$是预测值，$c(x)$是真实值，$E(h)$表示$h(x)$和$c(x)$相同的概率

###### 经典机器学习验证方法是用于正确性测试的最完善且使用最广泛的技术，包括交叉验证和引导

广泛采用的正确性度量有：准确性$(accuracy)$、精度$(precision)$、召回率$(recall)$和$(Area\ Under\ Curve, AUC)$

$accuracy$不区分其造成的错误类型$(False\ Positive|False\ Negative)$；当数据不平衡时，$precision$和$recall$可能会被误导，因此需要<u>仔细选择性能指标</u>

$Qin$等人提出从训练数据中生成一个镜像程序，然后使用该镜像程序的行为充当正确性预言

$Zhang$等人对$Tensorflow\ bug$相关的实证研究发现，在175个$bug$中，有40个$(22.9\%)$属于正确性问题

* ***模型相关性$(Model\ Relevance)$***

$f=|R(D,A)-R'(D,A')|$，$D$是训练数据，其中$R(D,A)$是算法$A$对模型$D$的预测能力，$R'(D,A')$是被测试的算法$A'$对模型$D$的预测能力

###### 即机器学习算法模型与数据之间的相关性，旨在研究算法对数据的拟合程度，较差的模型相关性通常是由过拟合$(overfitting)$或欠拟合$(underfitting)$引起的，过拟合和欠拟合都会导致模型对噪声不敏感

当模型的拟合程度过高，会将噪声也拟合，导致过拟合，训练数据不足时更容易过拟合

交叉验证是检测过拟合的传统方法，但并不能表示多少程度的过拟合是可以接受的，并且如果测试数据没有代表性的话，交叉验证很难测试过拟合

$Perturbed\ Model\ Validation(PMV)$：将噪声注入训练数据，针对干扰的数据重新训练模型，然后使用训练精度降低率来检测过度拟合/欠拟合

机器学习系统通常在部署后收集新数据作为训练数据以提高模型准确性，但是新收集的数据并不能作为测试数据，因此$Werpachowski$等人从测试数据中生成对抗样本，如果对抗样本的重新加权误差估计$(reweighted\ error\ estimate)$与原始测试集的值差异较大，则表明检测到过拟合

$Ma$等人重采样训练数据来解决过拟合问题，他们的方法将测试平均准确性平均从$75\%$提高到$93\%$

* ***鲁棒性和安全性$(Robustness\ and\ Security)$***

$r=E(S)-E(\delta(S))$，其中$S$是机器学习系统，$E(S)$是$S$的正确性，$\delta(S)$是存在扰动的机器学习系统，鲁棒性用来测量$E(S)$与$E(\delta(S))$之间的差异

###### 鲁棒性是机器学习系统的非功能性特征，一种衡量鲁棒性的方法是检查系统是否存在噪声，鲁棒性较好的在存在噪声的情况下仍能保持性能；安全性与鲁棒性密切相关，低鲁棒性会导致安全问题

对抗鲁棒性的扰动难以被检测，分为局部对抗鲁棒性$(Local\ Adversarial\ Robustness)$和全局对抗鲁棒性$(Global\ Adversarial\ Robustness)$

*局部对抗鲁棒性*：$\forall x':||x-x'||_p\le \delta \rightarrow h(x)=h(x')$

*全局对抗鲁棒性*：$\forall x,x':||x-x'||_p\le \delta \rightarrow h(x)=h(x')\le\epsilon$

其中$||\cdot||_p$是距离空间的$p$范式

局部对抗鲁棒性关注指定的一个输入的鲁棒性，全局对抗鲁棒性关注所有输入的鲁棒性

*1. 鲁棒性度量标准*

$DeepFool$：通过计算“欺骗”深度网络的扰动（添加的噪声），从而量化其鲁棒性

$Bastani$等人题注三种指标来度量鲁棒性：

1. 点向鲁棒性，表示分类器变得不鲁棒的最小输入变化
2. 对抗频率，表示更改输入会改变分类器结果的频率
3. 对抗严重性，表示输入与其最近的对抗样本之间的距离

$Carlini$和$Wagner$创建了一系列攻击，可用于构建神经网络鲁棒性的上边界

$Tjeng$等人提出使用测试输入与其最接近的对抗样本之间的距离来衡量鲁棒性

$Ruan$等人基于测试数据提供了全局鲁棒性的上下边界，以量化鲁棒性

$Gopinath$等人提出$DeepSafe$，一种用于评估$DNN$鲁棒性的数据驱动方法：聚类在同一簇中的输入有相同的输出标签

*2. 扰动测试数据*

$Carlini$和$Wagner$开发了使用距离度量来量化相似性的对抗样本生成方法

对抗样本生成被广泛应用于自动驾驶系统的鲁棒性测试，以及$NLI$模型和$DNC$

$Papernot$等人设计了一个库来标准化对抗样本构建的实现，因为“在没有标准化样本的情况下构建的$baseline$是无法相互比较的”：不能确定好结果是由高健壮性还是较高健壮性引起的

其他生成用于检查神经网络鲁棒性测试数据的技术包括：符号执行$(symbolic\ execution)$，模糊测试$(fuzz\ testing)$，组合测试$(combinatorial\ testing)$和抽象解释$(abstract\ interpretation)$

*3. 扰动整个系统*

$AVFI$：使用故障注入来估计自动驾驶系统的传感器、处理器或内存中的硬件错误，以测试鲁棒性

$Kayotee$：一种基于故障注入的工具，可以将故障系统地注入到自动驾驶系统的软件和硬件组件中

$DriveFI$：一种故障注入引擎，可挖掘对自动驾驶系统影响最大的情景和故障

* ***效率$(Efficiency)$***

$Zhang$等人对$Tensorflow\ bug$相关的实证研究发现，在175个$bug$中，有9个$(5.1\%)$属于效率问题

$Kirk$指出在训练模型时可以用不同算法的效率来比较他们的复杂度

$Spieker$和$Gotlieb$研究了三种训练数据减少$(Reduction)$方法，目的是从数据集中找到相同特征较少的子集，以提高机器学习测试的效率

* ***公平性$(Fairness)$***

###### 公平性研究的重点是衡量，发现，理解并应对有关不同群体或个人表现的差异

不公平性的主要原因：

1. 偏差样本$(Skewed\ Sample)$：一旦样本中存在初始偏差，则偏差可能会随着时间推移而加剧
2. 污染样本$(Tainted\ Examples)$：数据标签由于人类主观偏向而产生偏差
3. 有限的特征$(Limited\ Features)$：特征信息较少或提取错误，误导了特征与标签之间的联系
4. 样本规模差异$(Sample\ Size\ Disparity)$：如果来自$minority\ group$和$majority\ group$的数据高度不平衡，机器学习模型可能会减少$minority\ group$
5. 代理$(Proxies)$：某些特征代表敏感属性，即使排除了敏感属性，也会有其他的属性来代替，只要这些属性存在，就可能出现偏差

*1. 公平性定义及度量标准*

为了方便公式化说明，定义$X$表示个体集合，$Y$表示$X$中每个个体对应的标签集合，$h$表示机器学习模型的预测值，$A$是敏感属性集合，$Z$是其他属性

$p.s.$ 对**<u>敏感属性</u>**的理解为，对模型的决策输出值影响较大的属性，可能导致模型产生偏见的属性，比如在为用户推荐电影时，性别就是一个敏感属性，性别不同可能会导致决策输出值差异很大；对**<u>受保护属性</u>**的理解为，在模型训练过程中被排除在外的敏感属性，即不会因为这些属性而对决策结果产生偏见的属性

1. 无意识公平$(Fairness\ Through\ Unawareness)$：如果算法在决策过程中未显式使用受保护的属性，则算法是公平的。成本较低，但是当$X$中的非敏感属性与敏感属性间存在相关信息时会导致分歧，即使排除敏感属性也可能影响模型的准确性

2. 群体公平$(Group\ Fairness)$：如果基于敏感属性区分的群体的输出概率相同，则模型群体公平

    群体公平分为$Demographic\ Parity$和$Equalised\ Odds$

    $Demographic\ Parity$：要求模型的决策输出不受受保护属性的影响。定义$G_1$和$G_2$分别表示由某一敏感属性划分的两个样本，即$G_1$为受保护样本，$G_2$为非受保护样本，若满足条件$P\{h(x_i)=1|x_i\in G_1\}=P\{h(x_j)=1|x_j\in G_2\}$，则模型$h$满足$Demographic\ Parity$

    $p.s.$ 从保护样本中取出样本模型的决策结果为1的概率与从保护样本补集中取出样本模型的决策结果为1的概率相同

    $Equalised\ Odds$：当固定决策输出样本$Y$为$y_i$时，被测模型$h$不受受保护属性影响。即满足条件$P\{h(x_i)=1|x_i\in G_1,Y=y_i\}=P\{h(x_j)=1|x_j\in G_2,Y=y_i\}$

    $p.s.$ 即同时满足$P\{h(x_i)=1|x_i\in G_1,Y=1\}=P\{h(x_j)=1|x_j\in G_2,Y=1\}$和$P\{h(x_i)=1|x_i\in G_1,Y=0\}=P\{h(x_j)=1|x_j\in G_2,Y=0\}$，当$Y=1$时，称为模型$h$满足$Equal\ Opportunity$

3. 反事实公平性$(Counter-factual\ Fairness)$：将保护属性反转为一个反事实的值，并保持其他的属性不变，如果前后的模型决策输出结果相同，则满足反事实公平性。定义$a$为受保护属性，$a'$是$a$的反事实属性，$x'$是$a$变成$a'$后的输入，若满足条件$P\{h(x_i)_a=y_i|a\in A,x_i\in X\}=P\{h(x'_i)_{a'}=y_i|a'\in A,x_i\in X\}$，则模型$h$满足反事实公平性

    $p.s.$ 比如小明是一位女性，升职失败了，现将受保护属性“女性”翻转为反事实的”男性“时，求小明是否能升职。如果结果不变，则认为对于受保护属性“性别”不存在歧视，满足反事实公平性

4. 个体公平性$(Individual\ Fairness)$：相似的个体在模型中的决策输出结果也应相似。即当$d(x_i,x_j)<\epsilon$时，需要满足条件$P\{h(x_i)=y_i|x_i\in X\}=P\{h(x_j)=y_i|x_j\in X\}$，其中$d$用来衡量两个个体之间的相似性

$RobinHood$：一种旨在满足广泛的公平性约束条件的语境强盗算法，接受多个公平性的定义，并允许针对某个问题构建唯一的公平性定义。$p.s.$ 语境强盗算法是一种根据奖励来学习如何采取行动的算法

$Albarghouthi$和$Vinitsky$提出了“公平性意识编程”的概念，并开发了公平性规范语言，能够运行时监视代码

$Agarwal$等人将公平性分类简化为成本敏感分类的问题，这种简化优化了准确性和公平性约束间的权衡

$Albarghouthi$等人提出使用分布导向的归纳综合方法来修复决策程序

*2. 公平性测试的测试生成技术*

$Themis$：使用因果分析考虑<u>*群体公平性*</u>，定义了$Fairness\ Score$作为衡量公平性的标准，并使用随机测试生成技术评估歧视程度

$Aequitas$：对未覆盖的歧视性输入和对<u>*个体公平性*</u>较为重要的输入进行测试生成，首先对输入空间进行随机采样以发现歧视性输入，然后搜索这些输入的邻域以查找更多输入。$Aeqitas$还重新训练模型，以减少由这些模型做出的歧视决策

$Agarwal$等人结合符号执行与局部可解释性生成测试输入，使用局部解释识别影响决策的因素是否包括受保护的属性

$Tramer$等人提出“$fairness\ bug$”概念，认为受保护的属性与算法输出之间的统计显着性关联是一个公平性错误，并提出了一个综合测试工具，旨在通过“易于理解”的错误报告来帮助开发人员测试和调试公平性错误

$Sharma$和$Wehrheim$通过检查算法是否对训练数据的变化敏感来判断不公平性的原因，他们通过多种方式变异训练数据已生成新的数据集，如：改变行、列顺序，打乱特征和特征值等

* ***可解释性$(Interpretability)$***

即观察者可以理解机器学习系统做出决定的原因的程度，包括透明度$(transparency)$，即模型如何工作，和事后解释$(post\ hoc\ explanations)$，即可以从模型中获得的其他信息

*1. 人工评估可解释性$(Manual\ Accessment\ of\ Interpretability)$*

$Doshi$-$Velez$和$Kim$对可解释性的评估方法分为三类：基于应用程序$(application-grounded)$、基于人$(human-grounded)$、基于功能$(functionally-grounded)$， 基于应用程序的评估涉及对实际应用程序场景的人工实验；基于人的评估使用简化任务的人工评估结果；基于功能的评估不需要人工实验，而是使用定量度量解释质量

$Friedler$等人介绍了两种类型的可解释性：全局可解释性$(global\ interpretability)$表示理解整个已训练模型；本地可解释性$(local\ interpretability)$表示理解一个特定输入和相应输出的已训练模型的结果

*2. 自动评估可解释性$(Automatic\ Assessment\ of\ Interpretability)$*

$Cheng$等人提出一种度量标准，通过遮挡对象的周围环境来衡量模型是否在对象识别场景中学习了对象

$Christoph$提议根据机器学习算法的类别来衡量可解释性

$Zhou$等人定义了蜕变关系模式$(Metamorphic\ Relation\ Patterns,MRPs)$和蜕变关系输入模式$(Metamorphic\ Relation\ Input\ Patterns,MRIPs)$的概念，可以帮助用户理解机器学习系统的工作方式

* ***数据隐私$(Privacy)$***

机器学习系统保留私有数据信息的能力，使用差异隐私（differential privacy）来定义:$Pr[\mathcal{A}(D_1)\in S]\le exp(\epsilon)*Pr[\mathcal{A}(D_2)\in S]$，其中$\mathcal{A}$是随机算法，$D_1$、$D_2$是仅有一个实例不同的训练数据集，$S$是$\mathcal{A}$的子集，当上式成立时$\mathcal{A}$存在差异隐私。差异隐私反映了任何个人数据在以$\delta$为界的情况下是否会对结果产生显著影响

$Ding$等人将程序当做灰盒，并通过统计测试来检测不同的隐私侵害，对检测到的违规生成反例来说明，也能帮助开发人员理解和修复$bug$

$Bichsel$等人提出对差异隐私中的$\epsilon$参数进行估计，旨在找到侵犯隐私的可能性最大的三元组$(x,x',\Phi)$，其中$x$和$x'$是两个测试输入，$\Phi$是一组输出








